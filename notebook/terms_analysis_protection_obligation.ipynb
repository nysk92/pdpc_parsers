{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of Term Frequencies in PDPC Protection Obligation Decisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction import stop_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load documents into list for processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txts_path = Path(\"\") ###input path of folder containing Protection Obligation data already parsed to txt format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in txts_path.iterdir():\n",
    "    fname = str(x)\n",
    "    with open(fname) as f:\n",
    "        lines = f.readlines()\n",
    "        content = ''.join(lines).replace(\"\\n\", \"\")\n",
    "        docs.append(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_count = len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find terms with highest frequency in corpus\n",
    "#### In this example, we are trying to get common terms in protection obligation cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = CountVectorizer(ngram_range=(2,4), max_df=0.9).fit(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_words = vec.transform(docs)\n",
    "sum_words = bag_of_words.sum(axis=0) \n",
    "words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "\n",
    "words_freq = sorted(words_freq, key = lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 200 most frequent vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "words_freq[:200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define stopwords and words that do not help to identify the Protection Obligation topic, for later removal from vocabulary if element exclusively consists of such terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_ = list(stop_words.ENGLISH_STOP_WORDS)\n",
    "\n",
    "pdpa_stopwords = [\"data\", \"pdpa\", \"sgpdpc\", \n",
    "                  \"2019\", \"2018\", \"2017\", \n",
    "                  \"pte\", \"ltd\", \"did\", \"respect\", \"organisation\",\n",
    "                 \"yeong\",  \"zee\", \"kin\",  \"commissioner\", \"commission\",\n",
    "                 \"protection\", \"act\"]\n",
    "\n",
    "stopwords_ = stopwords_ +  pdpa_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_stopwords(phrase):\n",
    "    wrds = phrase.split(\" \")\n",
    "    if all(wrd in stopwords_ for wrd in wrds):\n",
    "        return True \n",
    "    else: return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine the vocabulary that occurs in the most amount of documents in the corpus\n",
    "\n",
    "#### This gives a list. of candidate keyphrases that can be used to identify the topic, validated by lawyer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_vocab = [vc for vc in list(vec.vocabulary_.keys()) if not all_stopwords(vc)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get vocabulary that appears in more than 70% of the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_across_doc_distrib = {}\n",
    "\n",
    "for v in all_vocab:\n",
    "    in_doc = 0\n",
    "    for doc in docs: \n",
    "        if re.search(v, doc, flags=re.I):\n",
    "            in_doc += 1\n",
    "    if in_doc != 0:\n",
    "        doc_coverage = in_doc/doc_count *  100\n",
    "    if doc_coverage > 70:\n",
    "        vocab_across_doc_distrib[v] = doc_coverage\n",
    "        \n",
    "# Needs to be optimised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_across_doc_distrib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "###  Get keyphrases in document using tfidf\n",
    "\n",
    "#### Allow discovery of subtopics and important factual elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer=TfidfVectorizer(use_idf=True, smooth_idf=True, ngram_range=(2,3))\n",
    " \n",
    "tfidf_vectorizer_vectors=tfidf_vectorizer.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the first vector out (for the first document)\n",
    "first_vector_tfidfvectorizer=tfidf_vectorizer_vectors[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# place tf-idf values in a pandas data frame\n",
    "df = pd.DataFrame(first_vector_tfidfvectorizer.T.todense(), index=tfidf_vectorizer.get_feature_names(), columns=[\"tfidf\"])\n",
    "df.sort_values(by=[\"tfidf\"],ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the top tfidf scoring terms in a particular document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_terms(doc_no):\n",
    "    nth_vector_tfidfvectorizer=tfidf_vectorizer_vectors[doc_no]\n",
    "    ndf = pd.DataFrame(nth_vector_tfidfvectorizer.T.todense(), index=tfidf_vectorizer.get_feature_names(), columns=[\"tfidf\"])\n",
    "    print(ndf.sort_values(by=[\"tfidf\"],ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_top_terms(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### References\n",
    "\n",
    "https://medium.com/@cristhianboujon/how-to-list-the-most-common-words-from-text-corpus-using-scikit-learn-dad4d0cab41d\n",
    "\n",
    "https://kavita-ganesan.com/tfidftransformer-tfidfvectorizer-usage-differences/#.XqWc9aep3yw\n",
    "\n",
    "https://stackoverflow.com/questions/34232190/scikit-learn-tfidfvectorizer-how-to-get-top-n-terms-with-highest-tf-idf-score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
